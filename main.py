from dotenv import load_dotenv
import os
from src.model.chat_model import MyChatBot
from langchain_core.messages import HumanMessage, SystemMessage
from src.document_store.documents_loader import DocumentStoring
from src.document_store.ingestion_pipeline import ingestion_pipeline
from src.task.retrieval import retrieve, generate
from src.state import State
from src.prompt.template import create_prompt
from src.langgraph_tracking import build_graph
# from app import initialize_rag_pipeline
import re

def initialize_vector_store():
    """
    Initialize and return the vector store with documents.
    Returns:
        vector_store: The initialized vector store instance
    """
    try:
        # Load environment variables
        load_dotenv("/Users/kittnguyen/Documents/education-rag/.env")
        print("‚úÖ Environment variables loaded successfully")
        
        # Initialize vector store
        print("üîÑ Initializing vector store...")
        vector_store = ingestion_pipeline()
        if vector_store is None:
            raise Exception("Failed to initialize vector store")
        print("‚úÖ Vector store initialized successfully")
        
        return vector_store
    except Exception as e:
        print(f"‚ùå Error during vector store initialization: {str(e)}")
        return None

def initialize_llm():
    """
    Initialize and return the LLM model.
    Returns:
        llm: The initialized LLM instance
    """
    try:
        print("üîÑ Initializing LLM...")
        llm = MyChatBot()
        print("‚úÖ LLM initialized successfully")
        return llm
    except Exception as e:
        print(f"‚ùå Error during LLM initialization: {str(e)}")
        return None

def initialize_rag_pipeline():
    """
    Initialize the complete RAG pipeline including LLM and vector store.
    Returns:
        tuple: (llm, vector_store) if successful, (None, None) if failed
    """
    try:
        # Initialize LLM
        llm = initialize_llm()
        if llm is None:
            return None, None

        # Initialize vector store
        vector_store = initialize_vector_store()
        if vector_store is None:
            return None, None
        
        return llm, vector_store
    except Exception as e:
        print(f"‚ùå Error during RAG pipeline initialization: {str(e)}")
        return None, None

def process_question(question: str, graph):
    """
    Process a question through the RAG pipeline with streaming.
    Args:
        question (str): The question to process
        graph: The RAG graph instance
    Returns:
        bool: True if successful, None if failed
    """
    try:
        # Process the question through the RAG pipeline with streaming
        print("\nüìù Answer:")
        for message, metadata in graph.stream(
            {"question": question}, 
            stream_mode="messages"
        ):
            if hasattr(message, 'content'):
                print(message.content, end="", flush=True)
        print()  # Add newline at the end
        return True
    except Exception as e:
        print(f"‚ùå Error processing question: {str(e)}")
        return None

def main():
    # Initialize the RAG pipeline
    llm, vector_store = initialize_rag_pipeline()
    if llm is None or vector_store is None:
        print("‚ùå Failed to initialize RAG pipeline")
        return

    # Build the RAG graph
    try:
        print("üîÑ Building RAG graph...")
        # Create functions that will be used in the graph
        def retrieve_node(state):
            return retrieve(state, vector_store=vector_store)
            
        def generate_node(state):
            return generate(state, llm=llm, create_prompt=create_prompt)
            
        graph = build_graph(
            State,
            retrieve_node,
            generate_node
        )
        print("‚úÖ RAG graph built successfully!")
    except Exception as e:
        print(f"‚ùå Error building RAG graph: {str(e)}")
        return

    # Interactive Q&A loop
    print("\nü§ñ RAG Pipeline is ready! Type 'exit' to quit.")
    while True:
        try:
            question = input("\n‚ùì Please enter your question: ").strip()
            
            if question.lower() == 'exit':
                print("üëã Goodbye!")
                break
                
            if not question:
                print("‚ö†Ô∏è Please enter a valid question")
                continue
                
            print(f"\nüîÑ Processing your question: {question}")
            process_question(question, graph)
                
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"‚ùå An error occurred: {str(e)}")

if __name__ == "__main__":
    main()

